# -*- coding: utf-8 -*-
"""FinalFinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OHRThWrJ28ODQngHuTPkNHMZzCMylhOD

#Importing Libraries
"""

!pip install --upgrade keras
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, accuracy_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold, GridSearchCV
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
import numpy as np
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf
from keras import initializers, layers
import keras
from sklearn.datasets import make_classification
from tensorflow import keras
!pip install keras-tuner --upgrade
from keras_tuner import Hyperband

"""# Loading Dataset"""

#tf.random.set_seed(0)#Setting Random Seed for fixed random

keras.utils.set_random_seed(812)

dataset = pd.read_csv('MAIN.csv')

"""# Data Preprocessing"""

data = dataset.iloc[:300,:7]# Get rid of blank spaces
print(data.isna().sum())# Initial Summary
print(data.head)
#Get Rid of year column and make it index column to work better with model
data.index = pd.to_datetime(data.Year,errors='coerce')
data = data.drop('Year', axis=1)

# Find NaN value
data.isna().sum()
data = data.fillna(method='ffill')#Forward Fill missing values
data = data.resample('M').mean()# Format index to YYYY-MM-DD
data.isna().sum()
data = data.fillna(method='ffill')
data.isna().sum()

futureData = data

data = data.iloc[:229,:]# Get rid of blank spaces

"""# Feature Correlation"""

sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.show()

plt.plot(data.index, data.iloc[:,2], label='Actual')


plt.title('Average Housing Price Windsor, ON 2005-2024')
plt.xlabel('Year')
plt.ylabel('Average Price')
plt.legend()
plt.ylim(100000, 800000)
plt.grid(True)
plt.show()

"""#Scale Data"""

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

"""# Create Sequence"""

seq_n = 1

X=[]
Y=[]
for i in range(len(scaled_data)-seq_n):
  X.append(scaled_data[i:i+seq_n, :])
  Y.append(scaled_data[i+seq_n, :])

X = np.array(X)
Y = np.array(Y)


print(Y)
X = X.reshape(1, 228, 6)
Y = Y.reshape(1, 228, 6)

"""# Create First LSTM Model"""

def encoder_model(hp):
  #Training Model
  model = Sequential()
  model.add(tf.keras.Input((None,6)))
  model.add(LSTM(128, return_sequences=True, activation=hp.Choice("activation", ['relu','tanh']) ) )#
  model.add(Dropout(0.2))
  model.add(LSTM(6, return_sequences=True))
  #model.add(Dense(6))
  learning_rate = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")
  model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['accuracy'])#
  return model

"""#Create Tuner and Find best parameters for loss"""

tuner = Hyperband(
    encoder_model,
    objective='loss',
    max_epochs=100,
    directory='my_dir',
    project_name='intro_to_kt')

tuner.search(X, Y, epochs=10, batch_size = 1)

"""#Train Model"""

# Get the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# Build the best model
best_model = tuner.hypermodel.build(best_hps)

# Train the best model
history = best_model.fit(X, Y, epochs=100, batch_size=1)

"""# Plot Loss

"""

plt.plot(history.history['loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')


best_model.summary()
print(best_hps.values)
plt.savefig("test.svg", format="svg", dpi=1200)
print()
plt.show()

"""# Evaluation Metrics

"""

from sklearn.metrics import mean_absolute_error, mean_squared_error

predictions = best_model.predict(X)

# Calculate evaluation metrics
mae = mean_absolute_error(Y.reshape(228, 6), predictions.reshape(228, 6))
mse = mean_squared_error(Y.reshape(228, 6), predictions.reshape(228, 6))
rmse = np.sqrt(mse)

print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)

"""# Plotting All predicted values subplots"""

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
for i,ax in enumerate(axes.flat):
    ax.plot(data.index[-228:], Y.reshape(228, 6)[:,i], label='Actual')
    ax.plot(data.index[-228:], predictions.reshape(228, 6)[:,i], label='Predicted')
    ax.set_title(f'{data.columns.tolist()[i]} Prediction vs Actual')
    ax.set_ylabel('Value')
    ax.set_xlabel('Years (2005-2024)')
    ax.legend()

plt.tight_layout()
plt.show()

best_model.summary()
print(best_hps.values)

image_format = 'svg' # e.g .png, .svg, etc.
image_name = 'featurePred.svg'

fig.savefig(image_name, format=image_format, dpi=1200)

"""# PLotting just unemployment pred"""

plt.plot(data.index[-228:], Y.reshape(228, 6)[:,0], label='Actual')
plt.plot(data.index[-228:], predictions.reshape(228, 6)[:,0], label='Predicted')
plt.title(f'{data.columns.tolist()[0]} Prediction vs Actual')
plt.ylabel('Value')
plt.xlabel('Years (2005-2024)')
plt.legend()

plt.tight_layout()


best_model.summary()
print(best_hps.values)

image_format = 'svg' # e.g .png, .svg, etc.
image_name = 'UnempfeaturePred.svg'

plt.savefig(image_name, format=image_format, dpi=1200)
plt.show()

"""# Create Prediction Model (Decoder)"""

newModel = Sequential()
newModel.add(tf.keras.Input(batch_size=1, shape=(None,6)))
newModel.add(LSTM(128, return_sequences=True, stateful=True))
newModel.add(Dropout(0.2))
newModel.add(LSTM(6, return_sequences=False, stateful=True))

#newModel.add(Dense(6,))

newModel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

newModel.set_weights(best_model.get_weights())

"""# Predict N timesteps in the future"""

#newModel.reset_states()
newModel.layers[0].reset_states()
#first = newModel.predict(X).reshape(1,1,6)

future_predictions = []


forward_n =  48# Time Steps
nextpred = newModel.predict(X)#.reshape(1,1,6)


for i in range(forward_n):
  X = np.append( X[:, 1:, :] , nextpred.reshape(1,1,6), axis=1)
  nextpred = newModel.predict(X, verbose=0)
  future_predictions.append(nextpred)

#newModel.layers[0].reset_states()

future_predictions = np.array(future_predictions)

future_predictions = future_predictions.reshape(forward_n, 6)
final = scaler.inverse_transform(future_predictions)#[:,2]

"""# Plot Forecasted Data"""

plt.plot(data.index[-228:], data.iloc[1:,2], label='Actual')
plt.plot(futureData.index[228:228+forward_n], final[:,2], label='Predicted')

plt.title('Average Price 4 Years Prediction vs Actual')
plt.xlabel('Time')
plt.ylabel('Average Price')
plt.legend()
#plt.ylim(250, 400)
plt.grid(True)

image_format = 'svg' # e.g .png, .svg, etc.
image_name = 'Future.svg'

plt.savefig(image_name, format=image_format, dpi=1200)



plt.show()

"""# PLot all forecasted data (subplots"""

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
for i,ax in enumerate(axes.flat):
    ax.plot(data.index[-228:], data.iloc[1:,i], label='Actual')
    ax.plot(futureData.index[228:228+forward_n], final[:,i], label='Predicted')
    ax.set_title(f'{data.columns.tolist()[i]} Prediction vs Actual')
    ax.set_ylabel('Value')
    ax.set_xlabel('Years (2005-2024)')
    ax.legend()

plt.tight_layout()
plt.show()

best_model.summary()
print(best_hps.values)

image_format = 'svg' # e.g .png, .svg, etc.
image_name = 'futurefeaturePred.svg'

fig.savefig(image_name, format=image_format, dpi=1200)

final[:,2]